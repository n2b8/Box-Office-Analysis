{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Movie Studio Analysis: Understanding Box Office Success\n",
    "## Business Understanding\n",
    "The company is planning to launch a new movie studio, but lacks experience in movie production. The goal of this analysis is to explore current trends in the film industry and provide actionable insights that can guide the studio's strategy. This analysis focuses on identifying factors that contribute to box office success, including genre, budget, release timing, and the impact of key personnel such as directors, actors, and writers.\n",
    "    "
   ],
   "id": "641b1d4bb8d17bf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Understanding\n",
    "The analysis is based on multiple datasets related to movie budgets, revenues, genres, release dates, and key personnel (directors, actors, and writers).\n",
    "\n",
    "These datasets include:\n",
    "   - **Box Office Mojo (BOM) Movie Gross**: Contains information on domestic and foreign box office revenues.\n",
    "   - **Rotten Tomatoes (RT) Movie Info**: Provides metadata about movies, including genres and release dates.\n",
    "   - **TMDB Movies**: Includes information on movie popularity and ratings.\n",
    "   - **IMDb Database**: Provides detailed information on directors, actors, writers, and other key personnel involved in the movies.\n",
    "    "
   ],
   "id": "a1ef895ddc939345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "In this section, we will load, clean, and merge the datasets to prepare them for analysis."
   ],
   "id": "1228e2f43b54fb5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T23:25:48.751418Z",
     "start_time": "2024-09-03T23:25:48.745200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ],
   "id": "2e6858b81267ff2",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:46:38.143429Z",
     "start_time": "2024-09-03T21:46:38.064486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the datasets\n",
    "tmdb_movies = pd.read_csv(\"zippedData/tmdb.movies.csv.gz\")\n",
    "tn_movies = pd.read_csv(\"zippedData/tn.movie_budgets.csv.gz\")"
   ],
   "id": "c63942a5f2739757",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:55:45.792657Z",
     "start_time": "2024-09-03T21:55:41.720294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading IMDb SQLite database\n",
    "\n",
    "# Unzip the sqlite db file if not already done\n",
    "with ZipFile(\"zippedData/im.db.zip\", 'r') as zObject:\n",
    "    zObject.extractall(\"zippedData/\")\n",
    "\n",
    "# Creating the connection\n",
    "conn = sqlite3.connect(\"zippedData/im.db\")\n",
    "\n",
    "# Loading data for directors, actors, and writers filtering for US movies in English\n",
    "\n",
    "# Queries\n",
    "query_directors = \"\"\"\n",
    "SELECT mb.*, mr.averagerating, mr.numvotes, p.primary_name, p.birth_year, p.death_year, p.primary_profession\n",
    "FROM movie_basics AS mb\n",
    "JOIN movie_ratings AS mr ON mb.movie_id = mr.movie_id\n",
    "JOIN principals AS pr ON mb.movie_id = pr.movie_id\n",
    "JOIN persons AS p ON pr.person_id = p.person_id\n",
    "JOIN movie_akas AS ma ON mb.movie_id = ma.movie_id\n",
    "WHERE ma.region = 'US'\n",
    "AND pr.category = 'director'\n",
    "AND ma.language = 'en';\n",
    "\"\"\"\n",
    "\n",
    "query_actors = \"\"\"\n",
    "SELECT mb.*, mr.averagerating, mr.numvotes, p.primary_name, p.birth_year, p.death_year, p.primary_profession\n",
    "FROM movie_basics AS mb\n",
    "JOIN movie_ratings AS mr ON mb.movie_id = mr.movie_id\n",
    "JOIN principals AS pr ON mb.movie_id = pr.movie_id\n",
    "JOIN persons AS p ON pr.person_id = p.person_id\n",
    "JOIN movie_akas AS ma ON mb.movie_id = ma.movie_id\n",
    "WHERE ma.region = 'US'\n",
    "AND pr.category = 'actor'\n",
    "AND ma.language = 'en';\n",
    "\"\"\"\n",
    "\n",
    "query_writers = \"\"\"\n",
    "SELECT mb.*, mr.averagerating, mr.numvotes, p.primary_name, p.birth_year, p.death_year, p.primary_profession\n",
    "FROM movie_basics AS mb\n",
    "JOIN movie_ratings AS mr ON mb.movie_id = mr.movie_id\n",
    "JOIN principals AS pr ON mb.movie_id = pr.movie_id\n",
    "JOIN persons AS p ON pr.person_id = p.person_id\n",
    "JOIN movie_akas AS ma ON mb.movie_id = ma.movie_id\n",
    "WHERE ma.region = 'US'\n",
    "AND pr.category = 'writer'\n",
    "AND ma.language = 'en';\n",
    "\"\"\"\n",
    "\n",
    "# Execute queries and assign to dataframes\n",
    "directors_merged = pd.read_sql_query(query_directors, conn)\n",
    "actors_merged = pd.read_sql_query(query_actors, conn)\n",
    "writers_merged = pd.read_sql_query(query_writers, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ],
   "id": "64dc33962b3f561f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "Here I'll go through the process of cleaning the data by handling missing values, removing duplicate information, recasting data types, and feature engineering"
   ],
   "id": "d16f5427cb9d8f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T23:04:46.008204Z",
     "start_time": "2024-09-03T23:04:45.945529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cleaning IMDb data\n",
    "def clean_imdb_data(df):\n",
    "    # Handle missing values (dropping rows with missing ratings or votes)\n",
    "    df = df.dropna(subset=['averagerating', 'numvotes', 'primary_name'])\n",
    "    \n",
    "    # Convert numvotes to integers\n",
    "    df['numvotes'] = df['numvotes'].astype(int)\n",
    "    \n",
    "    # Filter out movies with less than 1000 votes\n",
    "    df = df[df['numvotes'] >= 1000]\n",
    "    \n",
    "    return df\n",
    "\n",
    "directors_cleaned = clean_imdb_data(directors_merged)\n",
    "actors_cleaned = clean_imdb_data(actors_merged)\n",
    "writers_cleaned = clean_imdb_data(writers_merged)"
   ],
   "id": "72fb96118dd148f3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T23:07:14.540198Z",
     "start_time": "2024-09-03T23:07:14.467127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cleaning TMDB data\n",
    "def clean_tmdb_data(df):\n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=['popularity', 'vote_count', 'release_date'])\n",
    "    \n",
    "    # Convert release_date to datetime\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    \n",
    "    # Convert vote_count to integer\n",
    "    df['vote_count'] = df['vote_count'].astype(int)\n",
    "    \n",
    "    # Remove any duplicate values based on movie title and release date\n",
    "    df = df.drop_duplicates(subset=['title', 'release_date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "tmdb_cleaned = clean_tmdb_data(tmdb_movies)"
   ],
   "id": "befda6ff97ec26d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T23:16:19.520709Z",
     "start_time": "2024-09-03T23:16:19.474777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cleaning 'The Numbers' movie budgets data\n",
    "\n",
    "# Our selected data already has currency in the proper format but just in case I add more data later this will come in handy\n",
    "def clean_currency(x):\n",
    "    if isinstance(x, str):\n",
    "        return float(x.replace('$', '').replace(',', ''))\n",
    "    return x\n",
    "\n",
    "def clean_tn_movie_budgets(df):\n",
    "    # Apply the currency cleaning function to the budget and revenue columns\n",
    "    df['production_budget'] = df['production_budget'].apply(clean_currency)\n",
    "    df['domestic_gross'] = df['domestic_gross'].apply(clean_currency)\n",
    "    df['worldwide_gross'] = df['domestic_gross'].apply(clean_currency)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=['production_budget', 'domestic_gross', 'worldwide_gross'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "tn_movie_budgets_cleaned = clean_tn_movie_budgets(tn_movies)"
   ],
   "id": "225d60702903692f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T23:28:46.740918Z",
     "start_time": "2024-09-03T23:28:46.566835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merging some of the data together\n",
    "# Merge TMDB and TN Movie Budgets data based on title and release date\n",
    "merged_data = pd.merge(tmdb_cleaned, tn_movie_budgets_cleaned, left_on='title', right_on='movie')\n",
    "\n",
    "# Merge the result with RT Movie Info data based on the title\n",
    "merged_data = pd.merge(merged_data, rt_movie_info_cleaned, left_on='title', right_on='id')\n",
    "\n",
    "# Drop any duplicates and handle any final cleaning\n",
    "merged_data = merged_data.drop_duplicates(subset=['title', 'release_date'])"
   ],
   "id": "e7196431a699bc4e",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'title'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(tmdb_cleaned, tn_movie_budgets_cleaned, left_on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m, right_on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmovie\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Merge the result with RT Movie Info data based on the title\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmerged_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrt_movie_info_cleaned\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mright_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Drop any duplicates and handle any final cleaning\u001B[39;00m\n\u001B[1;32m      9\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mdrop_duplicates(subset\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelease_date\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/anaconda3/envs/learn-env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:170\u001B[0m, in \u001B[0;36mmerge\u001B[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _cross_merge(\n\u001B[1;32m    156\u001B[0m         left_df,\n\u001B[1;32m    157\u001B[0m         right_df,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    167\u001B[0m         copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[1;32m    168\u001B[0m     )\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 170\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[43m_MergeOperation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result(copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/learn-env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:807\u001B[0m, in \u001B[0;36m_MergeOperation.__init__\u001B[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[0m\n\u001B[1;32m    803\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_tolerance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft_join_keys)\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001B[39;00m\n\u001B[1;32m    806\u001B[0m \u001B[38;5;66;03m# to avoid incompatible dtypes\u001B[39;00m\n\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_coerce_merge_keys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m \u001B[38;5;66;03m# If argument passed to validate,\u001B[39;00m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;66;03m# check if columns specified as unique\u001B[39;00m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;66;03m# are in fact unique.\u001B[39;00m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/learn-env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1508\u001B[0m, in \u001B[0;36m_MergeOperation._maybe_coerce_merge_keys\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1502\u001B[0m     \u001B[38;5;66;03m# unless we are merging non-string-like with string-like\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m   1504\u001B[0m         inferred_left \u001B[38;5;129;01min\u001B[39;00m string_types \u001B[38;5;129;01mand\u001B[39;00m inferred_right \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string_types\n\u001B[1;32m   1505\u001B[0m     ) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   1506\u001B[0m         inferred_right \u001B[38;5;129;01min\u001B[39;00m string_types \u001B[38;5;129;01mand\u001B[39;00m inferred_left \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string_types\n\u001B[1;32m   1507\u001B[0m     ):\n\u001B[0;32m-> 1508\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;66;03m# datetimelikes must match exactly\u001B[39;00m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m needs_i8_conversion(lk\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m needs_i8_conversion(rk\u001B[38;5;241m.\u001B[39mdtype):\n",
      "\u001B[0;31mValueError\u001B[0m: You are trying to merge on object and int64 columns for key 'title'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hypothesis testing",
   "id": "ae64980d2c2eee0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Does genre significantly impact revenue?\n",
   "id": "7d1b8bfe5921ecbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
